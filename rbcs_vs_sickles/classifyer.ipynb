{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring constants\n",
    "TRAIN_VAL_FACTOR = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img_src(folder):\n",
    "    arr_img_src = []\n",
    "\n",
    "    for item in os.listdir(folder):\n",
    "        if item.endswith(\".jpg\"):\n",
    "            arr_img_src.append(f\"{folder}/{item}\")\n",
    "\n",
    "    return arr_img_src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dict_img_src):\n",
    "    pile = []\n",
    "\n",
    "    # We create a big pile of dicts with x and y\n",
    "    for key in [\"Healthy\", \"Sick\"]:\n",
    "        for img_src in dict_img_src[key]:\n",
    "            print(f\"Preprocessing {img_src}\")\n",
    "\n",
    "            if key == \"Healthy\":\n",
    "                label = int(1)\n",
    "            else:\n",
    "                label = int(0)\n",
    "            \n",
    "            arr_img = cv2.imread(img_src, cv2.IMREAD_REDUCED_GRAYSCALE_8)\n",
    "            arr_img = cv2.resize(arr_img, (255, 255)) / 255 # /255 to normalize this image data\n",
    "            pile.append(np.array([arr_img, label]))\n",
    "\n",
    "    # We shuffle the pile to create a random order of healthy and sick cells\n",
    "    random.shuffle(pile)\n",
    "\n",
    "    # We create and empty np array for x (data) and y (label)\n",
    "    # x_pile = np.empty((len(pile), 255, 255))\n",
    "    x_pile = np.empty((0, 255, 255))\n",
    "    y_pile = np.array([])\n",
    "\n",
    "    # The pile is an array of dicts\n",
    "    # We need to unpack this and place in the 3d np array for x and y\n",
    "    print(\"Formatting data to tensor matrix...\")\n",
    "    for i in pile:\n",
    "        x_pile = np.append(x_pile, [i[0]], axis=0)\n",
    "        y_pile = np.append(y_pile, i[1])\n",
    "  \n",
    "    # Now that we seperated the x and y, we create the training and validation sets\n",
    "    print(\"Creating train and test batches...\")\n",
    "\n",
    "    # # Training validation data and labels\n",
    "    x_train = x_pile[:int((1 - TRAIN_VAL_FACTOR) * len(x_pile))] # Images\n",
    "    y_train = y_pile[:int((1 - TRAIN_VAL_FACTOR) * len(y_pile))] # Labels\n",
    "\n",
    "    # Validation data and labels\n",
    "    x_val = x_pile[int((1 - TRAIN_VAL_FACTOR) * len(y_pile)):] # Images\n",
    "    y_val = y_pile[int((1 - TRAIN_VAL_FACTOR) * len(y_pile)):] # Labels\n",
    "\n",
    "    print(\"Done\")\n",
    "\n",
    "    return ((x_train, x_val), (y_train, y_val))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# Prepocessing all data into usable chunks for CNN\n",
    "dict_img_src = { \"Healthy\": load_img_src(\"healthy\"), \"Sick\": load_img_src(\"sick\") }\n",
    "(x_train, x_val), (y_train, y_val) = preprocess(dict_img_src)\n",
    "\n",
    "x_train = x_train.reshape((-1, 255, 255, 1)) # Reshaping to (length_data_set, width_img, height_img, channels)\n",
    "x_val = x_val.reshape((-1, 255, 255, 1))\n",
    "\n",
    "np.save(\"x_train\", x_train)\n",
    "np.save(\"x_val\", x_val)\n",
    "np.save(\"y_train\", y_train)\n",
    "np.save(\"y_val\", y_val)\n",
    "\n",
    "\n",
    "print(f\"x_train: {x_train.shape}\")\n",
    "print(f\"y_train: {y_train.shape}\")\n",
    "print(f\"x_val: {x_val.shape}\")\n",
    "print(f\"y_val: {y_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load(\"x_train.npy\")\n",
    "y_train = np.load(\"y_train.npy\")\n",
    "x_val = np.load(\"x_val.npy\")\n",
    "y_val = np.load(\"y_val.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining CNN model\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(5, 5), padding=\"same\", activation=\"relu\", input_shape=(255, 255, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(5, 5)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1500, activation=\"relu\"))\n",
    "model.add(Dense(750, activation=\"relu\"))\n",
    "model.add(Dense(350, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\")) # 1 for firing or not firing, sigmoid because\n",
    "                                          # binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the model using binary_crossentropy and the adam optimizer\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model using training data x and saving the metrics to the arr_metrics variable\n",
    "arr_metrics = model.fit(x_train, y_train, batch_size=25, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we evaluate the model using the validation data y\n",
    "model.evaluate(x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model in project root directory\n",
    "model.save(\"rbc_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(arr_metrics.history.keys())\n",
    "\n",
    "# Showing metrics saved in arr_metrics\n",
    "plt.plot(arr_metrics.history[\"accuracy\"])\n",
    "plt.plot(arr_metrics.history[\"val_accuracy\"])\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(arr_metrics.history[\"loss\"])\n",
    "plt.plot(arr_metrics.history[\"val_loss\"])\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.legend(\"Traing\", \"Val\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37564bit8153ed8c25bd45b7b4519484779c0773",
   "display_name": "Python 3.7.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}